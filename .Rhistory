library(patchwork)
mixed.lmer2 <- lmer(Met_Int ~ COV2 + (1|Subject), data = ss_data)
a <- summary(mixed.lmer2) # from DAN
r_sq <- r.squaredGLMM(mixed.lmer2)
r_sq <- r.squaredGLMM(mixed.lmer2)
View(r_sq)
View(r_sq)
a <- summary(mixed.lmer2) # from DAN
View(a)
View(a)
a <- summary(mixed.lmer2) # from DAN
r_sq <- r.squaredGLMM(mixed.lmer2)
a
r_sq
---
title: "Simulated LMER"
## setting up enviornment
```{r}
# https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/ provided a lot o
# basic code and inspiration for this. While journals won't allow citing a blog, this was a great
# resource
rm(list=ls())
library(tidyverse)
library(lme4)
library(emmeans)
library(MuMIn)
library(patchwork)
set.seed(5280)
# https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/ provided a lot o
# basic code and inspiration for this. While journals won't allow citing a blog, this was a great
# resource
rm(list=ls())
library(tidyverse)
library(lme4)
library(emmeans)
library(MuMIn)
library(patchwork)
set.seed(5280)
# set fixed effect parameters. https://journals.sagepub.com/doi/full/10.1177/2515245920965119
# set seed
set.seed(5280)
# set fixed effect parameters. https://journals.sagepub.com/doi/full/10.1177/2515245920965119
beta_0 <- 800 # intercept; i.e., the grand mean. this would be avg pwr
beta_1 <- 50 # slope; the effect of category this would be difference in pwr
# set random effect parameters
tau_0 <- 50 # by-subject random intercept sd. We assume this comes from a normal dist with  mean 0 and unknown SD. SD for random intercept by sub
omega_0 <- 1 # by-item random intercept sd for seated and standing. by condition random intercept
# set more random effect and error parameters
tau_1 <- 25 # by-subject random slope sd for random slopes
rho <- .3 # correlation between intercept and slope
sigma <- 30 # residual (error) sd
#But note that we are sampling two random effects for each subject s, a random intercept T0s and a random slope T1s. It is possible for these values to be positively or negatively correlated, in which case we should not sample them independently. For instance, perhaps people who are faster than average overall (negative random intercept) also show a smaller than average effect of the in-group/out-group manipulation (negative random slope) because they allocate less attention to the task. We can capture this by allowing for a small positive correlation between the two factors, rho, which we assign to be .2.
# overall model: RTsi=β0+T0s+O0i+(β1+T1s)Xi+esi. The response time for subject s on item i, RTsi, is decomposed into a population grand mean, β0; a by-subject random intercept, T0s; a by-item random interce
# set number of subjects and items
n_subj <- 16 # number of subjects
# pt, O0i; a fixed slope, β1; a by-subject random slope, T1s; and a trial-level residual, esi. Our data-generating process is fully determined by seven population parameters, all denoted by Greek letters: β0, β1, τ0, τ1, ρ, ω0, and σ (see Table 2). In the next section, we apply this data-generating process to simulate the sampling of subjects, items, and trials (encounters).
n_seated <- 6 # number of seated observations
n_standing <- 6 # number of standing observations
#We need to create a table listing each item i, which category it is in, and its random effect, O0i:
# simulate a sample of items
# total number of items = n_ingroup +n_outgroup
items <- data.frame(
item_id = seq_len(n_seated + n_standing),
category = rep(c("seated", "standing"), c(n_seated, n_standing)),
O_0i = rnorm(n = n_seated + n_seated, mean = 0, sd = omega_0)
)
# effect-code category. this encodes a predictor as to which category each sim belongs to. seated should be less pwr, standing higher
items$X_i <- recode(items$category, "seated" = -0.5, "standing" = +0.5)
#We will later multiply this effect-coded factor by the fixed effect of category (beta_1 = 50) to simulate data in which the powers differ by postrue
# simulate a sample of subjects
# calculate random intercept / random slope covariance
covar <- rho * tau_0 * tau_1
# put values into variance-covariance matrix
cov_mx <- matrix(c(tau_0^2, covar, covar, tau_1^2),
nrow = 2, byrow = TRUE)
# generate the by-subject random effects
subject_rfx <- MASS::mvrnorm(n = n_subj,
mu = c(T_0s = 0, T_1s = 0),
Sigma = cov_mx)
# combine with subject IDs
subjects <- data.frame(subj_id = seq_len(n_subj),subject_rfx)
# cross subject and item IDs; add an error term
# nrow(.) is the number of rows in the table
trials <- crossing(subjects, items) %>%
mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %>%
select(subj_id, item_id, category, X_i, everything())
# calculate the response variable
dat_sim <- trials %>%
mutate(pwr = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * X_i + e_si) %>%
select(subj_id, item_id, category, X_i, pwr)
# Visualize the data
p1 <- ggplot(data = dat_sim, aes(x = pwr, fill = category)) + geom_density() +
facet_wrap(~subj_id) + xlab('Power (W)') + ylab('Density') + ggtitle('LMEM Approach') #+
#scale_fill_grey()
ggplot(data = dat_sim, aes(x = category, y = pwr, fill = category)) + geom_boxplot() +
geom_point(position=position_jitterdodge())
dat_sim%>%
group_by(subj_id, category)%>%
summarize(
meanPow = mean(pwr)
) %>%
ggplot(aes(x = category, y = meanPow)) +
geom_point(aes(color=as.factor(subj_id))) +
geom_line(aes(group = subj_id, color = as.factor(subj_id) )) +
theme_bw()
p2 <- dat_sim%>%
group_by(subj_id, category)%>%
summarize(
meanPow = mean(pwr)
) %>%
ggplot(aes(x = category, y = meanPow)) +
geom_point(aes(color=category)) +
facet_wrap(~subj_id) + ylab('Mean Power (W)') +
xlab('Posture') + ggtitle('Averaged Data Approach') #+
#scale_color_grey()
# Figure 1
p1 | p2
experiments <- 1e4
randDat <- rnorm(1000, mean = 800, sd = 50)
# function to calculate row varuances
rowVars <- function(x, na.rm=F) {
# Vectorised version of variance filter
rowSums((x - rowMeans(x, na.rm=na.rm))^2, na.rm=na.rm) / (ncol(x) - 1)
}
meanVec = numeric(length = 30)
sdVec = numeric(length = 30)
# Repeat from 1:30 grabbing values from 1 through 30 samples from the above distribution
for (noReps in 1:30){
largeDat <- matrix(nrow = 1e4, ncol=noReps)
for (i in 1:experiments){
largeDat[i,1:noReps] <- sample(randDat, noReps, replace = TRUE)
}
rowMeans(largeDat)
rowVars(largeDat)
meanVec[noReps] <- mean( abs( (rep(mean(randDat),length(rowMeans)) - rowMeans(largeDat)) / sqrt(rowVars(largeDat)) ) )
sdVec[noReps] <- sd( abs( (rep(mean(randDat),length(rowMeans)) - rowMeans(largeDat)) / sqrt(rowVars(largeDat)) ) )
}
replicateStudy <- data.frame(meanVec, sdVec, c(1:30))
names(replicateStudy)[3] <- 'NumReps'
ggplot(replicateStudy, aes(x=NumReps, y = meanVec)) + geom_point() +
ylab('Sampling Error (|Z-Score|)') + xlab('Number of Replicates')
## to remove: subj 1 half of trials, subj
dat_mess <- read.csv('C:/Users/daniel.feeney/OneDrive - Boa Technology Inc/Desktop/Rethinking Neuro Data Manuscript/Code/Data/messDat.csv')
setwd("~/GitHub/RethinkingNeuroData")
# https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/ provided a lot o
# basic code and inspiration for this. While journals won't allow citing a blog, this was a great
# resource
rm(list=ls())
library(tidyverse)
library(lme4)
library(emmeans)
library(MuMIn)
library(patchwork)
set.seed(5280)
# set fixed effect parameters. https://journals.sagepub.com/doi/full/10.1177/2515245920965119
# set seed
set.seed(5280)
# set fixed effect parameters. https://journals.sagepub.com/doi/full/10.1177/2515245920965119
beta_0 <- 800 # intercept; i.e., the grand mean. this would be avg pwr
beta_1 <- 50 # slope; the effect of category this would be difference in pwr
# set random effect parameters
tau_0 <- 50 # by-subject random intercept sd. We assume this comes from a normal dist with  mean 0 and unknown SD. SD for random intercept by sub
omega_0 <- 1 # by-item random intercept sd for seated and standing. by condition random intercept
# set more random effect and error parameters
tau_1 <- 25 # by-subject random slope sd for random slopes
rho <- .3 # correlation between intercept and slope
sigma <- 30 # residual (error) sd
#But note that we are sampling two random effects for each subject s, a random intercept T0s and a random slope T1s. It is possible for these values to be positively or negatively correlated, in which case we should not sample them independently. For instance, perhaps people who are faster than average overall (negative random intercept) also show a smaller than average effect of the in-group/out-group manipulation (negative random slope) because they allocate less attention to the task. We can capture this by allowing for a small positive correlation between the two factors, rho, which we assign to be .2.
# overall model: RTsi=β0+T0s+O0i+(β1+T1s)Xi+esi. The response time for subject s on item i, RTsi, is decomposed into a population grand mean, β0; a by-subject random intercept, T0s; a by-item random interce
# set number of subjects and items
n_subj <- 16 # number of subjects
# pt, O0i; a fixed slope, β1; a by-subject random slope, T1s; and a trial-level residual, esi. Our data-generating process is fully determined by seven population parameters, all denoted by Greek letters: β0, β1, τ0, τ1, ρ, ω0, and σ (see Table 2). In the next section, we apply this data-generating process to simulate the sampling of subjects, items, and trials (encounters).
n_seated <- 6 # number of seated observations
n_standing <- 6 # number of standing observations
#We need to create a table listing each item i, which category it is in, and its random effect, O0i:
# simulate a sample of items
# total number of items = n_ingroup +n_outgroup
items <- data.frame(
item_id = seq_len(n_seated + n_standing),
category = rep(c("seated", "standing"), c(n_seated, n_standing)),
O_0i = rnorm(n = n_seated + n_seated, mean = 0, sd = omega_0)
)
# effect-code category. this encodes a predictor as to which category each sim belongs to. seated should be less pwr, standing higher
items$X_i <- recode(items$category, "seated" = -0.5, "standing" = +0.5)
#We will later multiply this effect-coded factor by the fixed effect of category (beta_1 = 50) to simulate data in which the powers differ by postrue
# simulate a sample of subjects
# calculate random intercept / random slope covariance
covar <- rho * tau_0 * tau_1
# put values into variance-covariance matrix
cov_mx <- matrix(c(tau_0^2, covar, covar, tau_1^2),
nrow = 2, byrow = TRUE)
# generate the by-subject random effects
subject_rfx <- MASS::mvrnorm(n = n_subj,
mu = c(T_0s = 0, T_1s = 0),
Sigma = cov_mx)
# combine with subject IDs
subjects <- data.frame(subj_id = seq_len(n_subj),subject_rfx)
# cross subject and item IDs; add an error term
# nrow(.) is the number of rows in the table
trials <- crossing(subjects, items) %>%
mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %>%
select(subj_id, item_id, category, X_i, everything())
# calculate the response variable
dat_sim <- trials %>%
mutate(pwr = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * X_i + e_si) %>%
select(subj_id, item_id, category, X_i, pwr)
# Visualize the data
p1 <- ggplot(data = dat_sim, aes(x = pwr, fill = category)) + geom_density() +
facet_wrap(~subj_id) + xlab('Power (W)') + ylab('Density') + ggtitle('LMEM Approach') #+
#scale_fill_grey()
ggplot(data = dat_sim, aes(x = category, y = pwr, fill = category)) + geom_boxplot() +
geom_point(position=position_jitterdodge())
dat_sim%>%
group_by(subj_id, category)%>%
summarize.group(
meanPow = mean(pwr)
) %>%
ggplot(aes(x = category, y = meanPow)) +
geom_point(aes(color=as.factor(subj_id))) +
geom_line(aes(group = subj_id, color = as.factor(subj_id) )) +
theme_bw()
experiments <- 1e4
randDat <- rnorm(1000, mean = 800, sd = 50)
# function to calculate row varuances
rowVars <- function(x, na.rm=F) {
# Vectorised version of variance filter
rowSums((x - rowMeans(x, na.rm=na.rm))^2, na.rm=na.rm) / (ncol(x) - 1)
}
meanVec = numeric(length = 30)
sdVec = numeric(length = 30)
# Repeat from 1:30 grabbing values from 1 through 30 samples from the above distribution
for (noReps in 1:30){
largeDat <- matrix(nrow = 1e4, ncol=noReps)
for (i in 1:experiments){
largeDat[i,1:noReps] <- sample(randDat, noReps, replace = TRUE)
}
rowMeans(largeDat)
rowVars(largeDat)
meanVec[noReps] <- mean( abs( (rep(mean(randDat),length(rowMeans)) - rowMeans(largeDat)) / sqrt(rowVars(largeDat)) ) )
sdVec[noReps] <- sd( abs( (rep(mean(randDat),length(rowMeans)) - rowMeans(largeDat)) / sqrt(rowVars(largeDat)) ) )
}
replicateStudy <- data.frame(meanVec, sdVec, c(1:30))
names(replicateStudy)[3] <- 'NumReps'
ggplot(replicateStudy, aes(x=NumReps, y = meanVec)) + geom_point() +
ylab('Sampling Error (|Z-Score|)') + xlab('Number of Replicates')
## to remove: subj 1 half of trials, subj
dat_mess <- read.csv('C:/Users/daniel.feeney/OneDrive - Boa Technology Inc/Desktop/Rethinking Neuro Data Manuscript/Code/Data/messDat.csv')
dat_mess <- read.csv('messDat.csv')
dat_unbal <- read.csv('unbalDat.csv')
dat_sim$Subject <- dat_sim$subj_id
# fit a linear mixed-effects model to data
mod_sim <- lmer(pwr ~ category + (1 + category | subj_id),
data = dat_sim)
# derive output from lmer
a <- summary(mod_sim, corr = FALSE)
r_sq <- r.squaredGLMM(mod_sim)
nullMod <- lmer(pwr ~ (category|subj_id),
data = dat_sim)
aovRes <- anova(mod_sim, nullMod)
aovRes$`Pr(>Chisq)`[2]
View(r_sq)
mixed.lmer2 <- lmer(Met_Int ~ COV2 + (1|Subject), data = ss_data)
## load the data and have a look at it
# https://ourcodingclub.github.io/tutorials/mixed-models/
library(tidyverse)
library(lme4)
library(emmeans)
library(MuMIn)
library(patchwork)
ss_data <- read_csv("stepping_stones_data.csv")
setwd("~/GitHub/CC-Linear-mixed-models")
setwd("~/GitHub/CC-Linear-mixed-models")
# https://ourcodingclub.github.io/tutorials/mixed-models/
library(tidyverse)
library(lme4)
library(emmeans)
library(MuMIn)
library(patchwork)
ss_data <- read_csv("stepping_stones_data.csv")
head(ss_data)
## Let's say we want to know how the body length affects test scores.
## Have a look at the data distribution:
hist(ss_data$Met_Int)  # somewhat normal, but not bimodal
## It is good practice to  standardise your explanatory variables before proceeding - you can use scale() to do that:
## this means that is is standardized toa  percentage of the distribution...
ss_data$COV2 <- scale(ss_data$COV)
## Back to our question: is Met_Int affected by COV?
###---- Fit all data in one analysis -----###
## One way to analyse this data would be to try fitting a linear model to all our data, ignoring the sites and the mountain ranges for now.
library(lme4)
library(dplyr)
basic.lm <- lm(Met_Int ~ COV2, data = ss_data)
summary(basic.lm)
## Let's plot the data with ggplot2
library(ggplot2)
ggplot(ss_data, aes(x = COV2, y = Met_Int)) +
geom_point()+
geom_smooth(method = "lm")
### Assumptions?
## Plot the residuals - the red line should be close to being flat, like the dashed grey line
plot(basic.lm, which = 1)  # not perfect, but look alright
## Have a quick look at the  qqplot too - point should ideally fall onto the diagonal dashed line
plot(basic.lm, which = 2)  # a bit off at the extremes, but that's often the case; again doesn't look too bad
## However, what about observation independence? Are our data independent?
## We collected multiple samples from 18 participants...
## It's perfectly plausible that the data from within each participant are more similar to each other than the data from different particiapnts - they are correlated. Pseudoreplication isn't our friend.
## Have a look at the data to see if above is true
boxplot(Met_Int ~ Subject, data = ss_data)  # certainly looks like something is going on here
## We could also plot it colouring points by subject
ggplot(ss_data, aes(x = COV2, y = Met_Int, colour = Subject))+
geom_point(size = 2)+
theme_classic()+
theme(legend.position = "none")
### WANT MORE COLOR ###
## From the above plots it looks like our mountain ranges vary both in the dragon body length and in their test scores. This confirms that our observations from within each of the ranges aren't independent. We can't ignore that.
## So what do we do?
###----- Run multiple analyses -----###
## We could run many separate analyses and fit a regression for each of the subjects.
## Lets have a quick look at the data split by subjects
## We use the facet_wrap to do that
ggplot(aes(COV2, Met_Int), data = ss_data) + geom_point() +
facet_wrap(~ Subject) +
xlab("COV") + ylab("Metabolic power")
##----- Modify the model -----###
## We want to use all the data, but account for the data coming from different subjects
## let's add subjects as a fixed effect to our basic.lm
Subjects.lm <- lm(Met_Int ~ COV2 + Subject, data = ss_data)
summary(Subjects.lm)
#### COV2 IS ACTUALLY VERY SIGNIFICANT
#### But some others are not significant. But let’s think about what we are doing here for a second.
# The above model is estimating the difference in Met_int between the subjects -
# we can see all of them in the model output returned by summary(). But we are not
# interested in quantifying Met_Int for each specific subject: we just want
# to know whether COV2 affects Met_Int and we want to simply control for the
# variation coming from the subjects. # This is what we refer to as
# “random factors” and so we arrive at mixed effects models. Ta-daa!
###----- Mixed effects models -----###
# A mixed model is a good choice here: it will allow us to use all the data we
# have (higher sample size) and account for the correlations between data coming
# from the subjects. We will also estimate fewer parameters and
# avoid problems with multiple comparisons that we would encounter while using
# separate regressions. We are going to work in lme4, so load the package
# (or use install.packages if you don’t have lme4 on your computer
# install.packages("sjstats")
library(lme4)
library(sjstats)
## Fixed and random effects
# Fixed are variables that we expect to have an impact on the dependent variable
# (metabolic cost). They are explanatory (in SLR). Want to make
# conclusions about COV2 and Metabolic cost. COV is fixed and Met_Int is dependent
# random effects are grouping factors that we try to control. Always categorical
# we cant force R to make these effects cotniuous. We ar enot intersted in impact per
# say, but we know they might be influencing
# data for this sample could just be SAMPLE OF ALL POSSIBILITIES. E.g., with
# unlimited time and funding we could have sampled each subject across the world, but
# we are needing to generalize. We don't care that Subject 03 and 04 are differnet,
# but we know that there might be something (i.e. technique, vision, rx time) about them that is making things diferent
# and we would like to know how much variation is attributed to THEM when we predict
# metabolic cost for people "off the street"
# Our data is just a sample of these 18 subjects. We hope that are results are genrealizable
# to people "off the street, however, we know that metabolic cost within a subject might
# be correlated, so we want to account for that.
# IF WE CHOSE THESE SUBJECTS PRIORI, and were interested in these
# SPECIFIC PEOPLE and wanted to make predictions ABOUT THEM, it would be FITTED AS A
# FIXED effect.
# should this be fixed or random? Data is for 4 DP for each participant...
# fits each subject with their own intercept...clear
mixed.lmer2 <- lmer(Met_Int ~ COV2 + (1|Subject), data = ss_data)
a <- summary(mixed.lmer2) # from DAN
r_sq <- r.squaredGLMM(mixed.lmer2)
mixed.lmer2 <- lmer(Met_Int ~ COV2 + (1 + COV2|Subject), data = ss_data)
mixed.lmer2 <- lmer(Met_Int ~ COV2 + (1 + COV2|Subject), data = ss_data)
a <- summary(mixed.lmer2) # from DAN
r_sq <- r.squaredGLMM(mixed.lmer2)
r_Sq
r_sq <- r.squaredGLMM(mixed.lmer2)
r_sq
mixed.lmer2 <- lmer(Met_Int ~ COV2 + (1 + COV2|Subject), data = ss_data)
a <- summary(mixed.lmer2) # from DAN
r_sq <- r.squaredGLMM(mixed.lmer2)
summary(a)
setwd("~/GitHub/RethinkingNeuroData")
mod_sim <- lmer(pwr ~ category + (1 + category | subj_id),
data = dat_sim)
# derive output from lmer
a <- summary(mod_sim, corr = FALSE)
r_sq <- r.squaredGLMM(mod_sim)
r_sq
nullMod <- lmer(pwr ~ (category|subj_id),
data = dat_sim)
aovRes <- anova(mod_sim, nullMod)
aovRes$`Pr(>Chisq)`[2]
avgDat <- dat_sim %>%
group_by(subj_id, category) %>%
summarize( meanPwr = mean(pwr))
t.test(meanPwr ~ category, data = avgDat, paired = TRUE)
```
```{r}
#mod_sim
df_partial_pooling <- coef(mod_sim)[["subj_id"]] %>%
rownames_to_column("Subject") %>%
as_tibble() %>%
rename(Intercept = `(Intercept)`, Slope_Cond = categorystanding) %>%
add_column(Model = "Partial pooling")
df_partial_pooling$Subject <- as.integer(df_partial_pooling$Subject)
df_models <- bind_rows(df_pooled, df_no_pooling, df_partial_pooling) %>%
left_join(dat_sim, by = "Subject")
# https://www.linguisticsociety.org/sites/default/files/e-learning/class_3_slides.pdf
# Join the raw data so we can use plot the points and the lines.
df_models <- bind_rows(df_pooled, df_no_pooling) %>%
left_join(dat_sim, by = "Subject")
ggplot(data = dat_sim, mapping=aes(x=category, y=pwr, color = category))+
geom_point() + facet_wrap(~subj_id)
# fit a no pooling model
no_pooling_mod <- lmList(pwr ~ category | subj_id, dat_sim)
df_no_pooling <- lmList(pwr ~ category | subj_id, dat_sim) %>%
coef() %>%
# Subject IDs are stored as row-names. Make them an explicit column
rownames_to_column("Subject") %>%
rename(Intercept = `(Intercept)`, Slope_Cond = categorystanding) %>%
add_column(Model = "No pooling")
df_no_pooling$Subject <- as.integer(df_no_pooling$Subject)
#fit a complete pooling mode
m_pooled <- lm(pwr ~ category, dat_sim)
# Repeat the intercept and slope terms for each participant
df_pooled <- tibble(
Model = "Complete pooling",
Subject = unique(dat_sim$subj_id),
Intercept = coef(m_pooled)[1],
Slope_Cond = coef(m_pooled)[2]
)
head(df_pooled)
# https://www.linguisticsociety.org/sites/default/files/e-learning/class_3_slides.pdf
# Join the raw data so we can use plot the points and the lines.
df_models <- bind_rows(df_pooled, df_no_pooling) %>%
left_join(dat_sim, by = "Subject")
p_model_comparison <- ggplot(df_models) +
aes(x = category, y = pwr) + ylab('Power (W)') +
# Set the color mapping in this layer so the points don't get a color
geom_abline(
aes(intercept = Intercept, slope = Slope_Cond, color = Model, linetype = Model),
size = .75
) +
geom_point() +
facet_wrap("Subject") +
theme(legend.position = "top", legend.justification = "left") +
scale_color_grey()
p_model_comparison
# https://www.linguisticsociety.org/sites/default/files/e-learning/class_3_slides.pdf
# Join the raw data so we can use plot the points and the lines.
df_models <- bind_rows(df_pooled, df_no_pooling) %>%
left_join(dat_sim, by = "Subject")
p_model_comparison <- ggplot(df_models) +
aes(x = category, y = pwr) + ylab('Power (W)') +
# Set the color mapping in this layer so the points don't get a color
geom_abline(
aes(intercept = Intercept, slope = Slope_Cond, color = Model, linetype = Model),
size = .75
) +
geom_point() +
facet_wrap("Subject") +
theme(legend.position = "top", legend.justification = "left") +
scale_color_grey()
p_model_comparison
# https://www.linguisticsociety.org/sites/default/files/e-learning/class_3_slides.pdf
# Join the raw data so we can use plot the points and the lines.
df_models <- bind_rows(df_pooled, df_no_pooling) %>%
left_join(dat_sim, by = "Subject")
p_model_comparison <- ggplot(df_models) +
aes(x = category, y = pwr) + ylab('Power (W)') +
# Set the color mapping in this layer so the points don't get a color
geom_abline(
aes(intercept = Intercept, slope = Slope_Cond, color = Model, linetype = Model),
size = .75
) +
geom_point() +
facet_wrap("Subject") +
theme(legend.position = "top", legend.justification = "left") +
scale_color_grey()
p_model_comparison
# https://www.linguisticsociety.org/sites/default/files/e-learning/class_3_slides.pdf
# Join the raw data so we can use plot the points and the lines.
df_models <- bind_rows(df_pooled, df_no_pooling) %>%
left_join(dat_sim, by = "Subject")
p_model_comparison <- ggplot(df_models) +
aes(x = category, y = pwr) + ylab('Power (W)') +
# Set the color mapping in this layer so the points don't get a color
geom_abline(
aes(intercept = Intercept, slope = Slope_Cond, color = Model, linetype = Model),
size = .75
) +
geom_point() +
facet_wrap("Subject") +
theme(legend.position = "top", legend.justification = "left") +
scale_color_grey()
p_model_comparison
# https://www.linguisticsociety.org/sites/default/files/e-learning/class_3_slides.pdf
# Join the raw data so we can use plot the points and the lines.
df_models <- bind_rows(df_pooled, df_no_pooling) %>%
left_join(dat_sim, by = "Subject")
p_model_comparison <- ggplot(df_models) +
aes(x = category, y = pwr) + ylab('Power (W)') +
# Set the color mapping in this layer so the points don't get a color
geom_abline(
aes(intercept = Intercept, slope = Slope_Cond, color = Model, linetype = Model),
size = .75
) +
geom_point() +
facet_wrap("Subject") +
theme(legend.position = "top", legend.justification = "left") +
scale_color_grey()
p_model_comparison
